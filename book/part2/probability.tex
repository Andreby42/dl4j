\chapter{概率与分布}
\label{chap:regression}

日常生活中，我们差不多一直在做各种判断，这样或者那样。俗话说，选择大于努力。如果你的认知或者价值观是错误的，将不会得出正确的结果。
这种价值观的偏离，需要“\emph{误差分析}”保证在正确的轨道上。然而，误差很难线性（任意曲线）吻合的，这取决于个人的生活经验，

\begin{figure}[!htb]
\centerline{\includegraphics[width=.1\figwidth]{images/4or9.png}}
\centerline{手写数字：4还是9？}
\end{figure}

如上图，手写数字很不容易判断，只能根据大多数人的情况猜测。
如果，你能拿到该人之前的手稿了解一下，就能有99.999\%概率做出正确判断。

有很多情形，我们也很难正确判断，只需要\emph{人工智能}输出TOPN可能性。
譬如，仅根据病人是否发烧、咳嗽难以确诊是否肺炎。进一步，借助沟通和血液化验，才更加确诊病人的实际病症。
这也只能说是99.99\%的准确率，即使是非常有经验的医生也有误诊的概率，只是非常小而已。

\emph{人工智能}研究的多是不确定性问题，需要你掌握概率和统计学的基础知识。
现实生活中，很多不确定事件都服从某种分布，譬如公交地铁站客流量服从泊松分布，而
1小时内到银行办理业务的客户数都也服从泊松分布（Poisson distribution）。
泊松分布具有以下特点：
\begin{itemize}
\item[1.] 无限分隔为若干小时间段，在这接近于零的小时间段里，发生1次的概率与时间段的长度成正比。
\item[2.] 在每一个极小时间段内，该事件发生两次及以上的概率恒等于零。
\item[3.] 在不同的小时间段里，发生与否相互独立。
\end{itemize}
\vspace{0.3cm}
\begin{equation*}
P\left( x \right) = \frac{{e^{ - \lambda } \lambda ^x }}{{x!}}
\end{equation*}

\noindent
泊松分布服从上述公式，很显然它所有事件的概率和等于1。
\begin{equation*}
\frac{{e^{-\lambda}\lambda^x }}{{x!}}
=e^{-\lambda }\left( \frac{{ \lambda ^x }}{{x!}}\right)
=e^{-\lambda } * e^\lambda
=1
\end{equation*}

\begin{lstlisting}[language=java]
在医疗质量上，美国误诊率是30-40%之间，因医疗差错而死亡的人，仅次于心血管疾病与肿瘤。
个别疾病的误诊率高得使你不敢想象，达到70%以上。
我国的误诊率尚没有公认的数据，但与美国相比只会高不会低。
\end{lstlisting}

\section{全概率公式}

全概率公式是概率论中重要的公式。他的意义在于直接计算$P(A)$比较困难时，可以将事件$A$分解为几个
小事件$B_i$，通过计算这些小事件的概率和，从而达到简化问题的效果。


全概率公式可以用如下的公式表示：
\begin{equation}
    P(A)=\sum_{i=1}^{\infty} P\left(B_{i}\right) P\left(A | B_{i}\right)
\end{equation}

\noindent
其中，事件$B_1,B_2,B_3,...,B_n$需要满足完备事件组，
即两两之间不能有交集，它们的和为全集，且概率大于零。
这样，事件$A$就被分解成了小事件$AB_1,AB_2,AB_3,...,AB_n$，由概率的加法公式即可得出
事件$A$的概率。


与全概率公式相反，贝叶斯公式是基于$P(A)$已知的前提下，寻找事件发生的原因$P(B_i|A)$。
贝叶斯公式可以用以下公式表示：
\begin{equation*}
    P\left(B_{t} | A\right)=\frac{P\left(B_{i}\right) P\left(A | B_{i}\right)}{\sum_{j=1}^{n} P\left(B_{j}\right) P\left(A | B_{j}\right)}
\end{equation*}

\noindent
其中事件$B_1,B_2,B_3,...,B_n$须满足完备事件组。

\section{最大似然}

最大似然估计是一种统计方法，用来求一个样本集的相关概率密度函数的参数。通俗地来讲，
就是通过已知的样本结果，反推最有可能出现这样结果的参数值。

例如，假设一个口袋中同时存在黑球和白球，我们从中随机抽取十个球，得到了8个黑球和2个白球。
在求解最有可能的黑白球比例时，我们就会采用最大似然法：假设从中抽到黑球的概率为$p$，
那么得到8次黑球2次白球的概率为：

\begin{equation*}
  P(A)=p^{8}*(1-p)^2
\end{equation*}

\noindent
在这个公式中，使$P(A)$达到最大的$p$值即为我们要求的结果。这就是最大似然问题的基本过程。


\section{期望与方差}

期望也称数学期望，在概率论与数理统计中指的是一个离散型随机变量在实验中每次可能的结果乘上
各自的概率的总和。在机器学习中，期望值是衡量一组数据离散程度的重要度量。

期望值的计算用以下公式表达$\mathrm{E}[X]=\sum_{i} p_{i} x_{i}$，其中$\mathrm{E}[X]$代表
期望值，$x_{i}$和$p_{i}$分别代表每次随机变量实验的可能结果与出现的概率。

与期望值类似，方差是概率论中衡量随机变量离散程度的度量。它具体指每个样本值与全体样本值平均数
之差的平方值的平均数。一般情况下，方差的公式可以用如下公式定义：$\sigma^{2}=\frac{\sum(X-\mu)^{2}}{N}$；其中，
$\sigma^{2}$为总体方差，$X$为单个样本值，$\mu$为总体样本的平均值，$N$为总体样本的个数。

同时，方差还可以用期望值来求出$\operatorname{Var}(X)=\mathrm{E}\left[X^{2}\right]-(\mathrm{E}[X])^{2}$
，由此可见期望值与方差之间紧密的联系。


\section{常见概率分布}
本书不打算把所有的概率知识进行讲解，主要为后续章节做铺垫。生活中，常见的概率事件都是离散的，譬如泊松分布。
\begin{equation}
P(X=x_i) = p_i, i=1,2,...n
\end{equation}
且概率$P_i$满足$\sum\limits_{i=1}^{n}P_i=1$。因此，离散型随机变量X的概率分布函数为，其中$x_i$为可能的状态。
\begin{equation}
F(x) = \sum\limits_{x_i<n}P_i.
\end{equation}

\noindent
这是离散型概率的特征，具备以下特征：
\begin{itemize}
\item[1.] $P(x_i) = 0$代表不会发生，$P(x_i) = 1$表示一定会发生。
\item[2.] 总概率不会大于1，也就是$F(x)<=1$
\end{itemize}

概率分布函数是概率论的重要概念，在实际应用中常用的有正态分布函数、泊松分布函数、二项分布函数等。
对于离散型随机变量，分布函数是“0-1分布”、“二项式分布”、“泊松分布”等；
而连续型随机变量有“均匀分布”、“正态分布”、“瑞利分布”等。

\subsection{正态分布}
正态分布，也称常态分布，高斯分布，是一种概率分布模型。正态分布在数学，工程与物理领域有着
重要的意义。在机器学习中，正态分布的统计模型应用非常广泛。

若一个随机变量$X$服从位置参数为$\mu$，尺度参数为$\sigma$的概率分布，且公式为
\begin{equation*}
  X \sim N\left(\mu, \sigma^{2}\right)
\end{equation*}
\noindent
则称变量$X$满足正态分布。

正态分布的概率密度函数则表示为
\begin{equation*}
  f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
\end{equation*}

\noindent
值得注意的是正态分布的数学期望值即为位置参数$\mu$，标准差即为尺度参数$\sigma$。



\section{矩估计}

矩估计又称矩法估计，指的是利用样本矩来估计总体所对应的参数。矩估计在概率论中有着
广泛的应用，主要的理论基础有大数定理和中心极限定理。

\subsection{大数定理}

大数定理是描述实验次数很大时所呈现的概率性质的定律。在随机事件大量重复出现的情况下，
事件往往会呈现出近乎必然的概率，这就是大数定律最简单的解释。


大数定律在高等数学上有以下两种数学解释：切比雪夫大数定理和伯努利大数定理。切比雪夫大数定理
假设有$x_1,x_2,x_3,...,x_n,...$这样一列相互独立的随机变量，那么对于任意小的正数$\xi$，有公式：
\begin{equation*}
  \lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{k=1}^{n} x_{k}-\frac{1}{n} \sum_{k=1}^{n} E x_{k}\right|<\varepsilon\right\}=1
\end{equation*}

\noindent
切比雪夫大数定理可以有以下理解：当样本容量$n$不断增大，样本的平均值将不断接近总体的
平均值。


伯努利大数定理则对大数定理进行了概率的解释，假设$\mu_n$是独立事件中事件$A$发生的次数，
则对于任意小的正数$xi$，满足公式：
\begin{equation*}
  \lim _{n \rightarrow \infty} P\left(\left|\frac{\mu_{n}}{n}-p\right|<\varepsilon\right)=1
\end{equation*}

\noindent
伯努利大数定理可以这样解释：当事件发生次数$n$足够大时，事件$A$发生频率将接近其发生的频率。

\subsection{中心极限定理}

中心极限定理与大数定理一样，都是描述试验次数很大时，所呈现的概率性质定理。但与大数定理不同的是，
中心极限定理描述的是随机事件大量重复，结果服从正态分布的情况。


假设有随机变量$X_1,X_2,...,X_n,...$相互独立，并具有方差和期望值，则对任意$x$有$F_n(x)$
\begin{equation*}
  F_{n}(x)=P\left\{\frac{\sum_{i=1}^{n} X_{i}-n \mu}{\sigma \sqrt{n}} \leq x\right\}
\end{equation*}

\noindent
利用中心极限定理和大数定理，我们可以利用少量数据对总体进行精确的预测，从而达到矩估计的效果。


